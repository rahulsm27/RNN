{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"hindi_english_parallel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               hindi  \\\n",
       "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "\n",
       "                                          english  \n",
       "0  Give your application an accessibility workout  \n",
       "1               Accerciser Accessibility Explorer  \n",
       "2  The default plugin layout for the bottom panel  \n",
       "3     The default plugin layout for the top panel  \n",
       "4  A list of plugins that are disabled by default  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hindi'] = df['hindi'].astype(str)\n",
    "df['english'] = df['english'].astype(str)\n",
    "\n",
    "df['hindi'] =df['hindi'].apply(lambda x : '[sos] ' + x + ' [eos]')\n",
    "df['english'] = df['english'].apply(lambda x : '[sos] ' + x + ' [eos]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[sos] Give your application an accessibility workout [eos]'\n",
      "b'[sos] Accerciser Accessibility Explorer [eos]'\n",
      "b'[sos] The default plugin layout for the bottom panel [eos]'\n",
      "b'[sos] The default plugin layout for the top panel [eos]'\n",
      "b'[sos] A list of plugins that are disabled by default [eos]'\n"
     ]
    }
   ],
   "source": [
    "text_dataset_en = tf.data.Dataset.from_tensor_slices(df['english'])\n",
    "\n",
    "\n",
    "for ele in text_dataset_en.take(5) :\n",
    "    print(ele.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000  # Maximum vocab size.\n",
    "max_len = 20  # Sequence length to pad the outputs to.\n",
    "\n",
    "\n",
    "# Create the layer.\n",
    "\n",
    "vectorize_layer_en= tf.keras.layers.TextVectorization(  \n",
    " max_tokens=max_features,\n",
    " standardize= \"lower_and_strip_punctuation\" ,\n",
    " output_mode ='int',\n",
    " encoding='utf-8',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "vectorize_layer_en.adapt(text_dataset_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorize_layer_en.get_vocabulary()))\n",
    "en_vocab = np.array( vectorize_layer_en.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorize_layer_en(df['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 1521,  994, 2458,    3,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,    4,  105,   75,  376,   11,    4,  654,  874,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,    4,  105,   75,  376,   11,    4,  401,  874,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   2,    7,   39,    8,  212,   62,   48,  741,   59,  105,    3,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.numpy()[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sos', 'accerciser', 'accessibility', 'explorer', 'eos', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', '', ''], dtype='<U20')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[X_train.numpy()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000  # Maximum vocab size.\n",
    "max_len = 20  # Sequence length to pad the outputs to.\n",
    "\n",
    "\n",
    "text_dataset_hindi = tf.data.Dataset.from_tensor_slices(df['hindi'])\n",
    "\n",
    "# Create the layer.\n",
    "\n",
    "vectorize_layer_hindi= tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    "# standardize=tf_lower_and_split_punct,\n",
    " output_mode='int',\n",
    " encoding='utf-8',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "vectorize_layer_hindi.adapt(text_dataset_hindi)\n",
    "\n",
    "print(len(vectorize_layer_hindi.get_vocabulary()))\n",
    "hindi_vocab = np.array( vectorize_layer_hindi.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sos', 'एक्सेर्साइसर', 'पहुंचनीयता', 'अन्वेषक', 'eos', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', ''], dtype='<U26')"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = vectorize_layer_hindi(df['hindi'])\n",
    "Y_train.numpy()[1:5]\n",
    "hindi_vocab[Y_train.numpy()[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [sos] अपने अनुप्रयोग को पहुंचनीयता व्यायाम का ...\n",
       "1          [sos] एक्सेर्साइसर पहुंचनीयता अन्वेषक [eos]\n",
       "2    [sos] निचले पटल के लिए डिफोल्ट प्लग-इन खाका [eos]\n",
       "3     [sos] ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका [eos]\n",
       "4    [sos] उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप...\n",
       "Name: hindi, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hindi'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Bidirectional, Concatenate, Dot, LSTM, multiply, RepeatVector, Dense, Embedding, Activation, Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 20\n",
    "Ty = 20\n",
    "embed_dim = 256\n",
    "vocab_size = 5000 # Max vocab size\n",
    "n_a = 64 # number of neurons in pre attention layer\n",
    "n_s =128 # number of neruons in post attention layer\n",
    "\n",
    "# Please note, this is the post attention LSTM cell.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
    "output_layer = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "def one_step_act(a,s_prev):\n",
    "    \n",
    "    s_prev = RepeatVector(Tx)(s_prev) # Shape M,Tx,n_s\n",
    "    concat = Concatenate(axis = -1)([a,s_prev])\n",
    "\n",
    "    e = Dense(10,activation = \"tanh\")(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = Dense(1,activation = \"relu\")(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = Activation(activation = 'softmax', name='attention_weights', axis = -1)(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\", in this order, to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = Dot(axes = 1)([alphas,a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "def Model_Trans(input):\n",
    "\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    # initial cell state\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    # hidden state\n",
    "    s = s0\n",
    "    # cell state\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "\n",
    "    X = Embedding(input_dim =vocab_size ,output_dim = embed_dim ,input_length = Tx )(input)\n",
    "    X = Bidirectional(LSTM(units = n_a,return_sequences=True))(X)\n",
    "\n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context =one_step_act(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector. (≈ 1 line)\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] \n",
    "        # Remember: s = hidden state, c = cell state\n",
    "        _, s, c = post_activation_LSTM_cell(inputs=context, initial_state=[s,c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "\n",
    "        # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model(inputs = [X,s0,c0],outputs= outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_Trans(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
